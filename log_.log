Running target_layer_idx=2
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Processing:   0%|                                                          | 0/1000 [00:00<?, ?it/s]/ai/teacher/ssz/layer_task/transformers/src/transformers/generation/configuration_utils.py:788: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Processing:   0%|                                                | 1/1000 [00:21<5:56:19, 21.40s/it]Processing:   0%|                                                | 2/1000 [00:42<5:52:33, 21.20s/it]Processing:   0%|▏                                               | 3/1000 [01:03<5:49:43, 21.05s/it]Processing:   0%|▏                                               | 4/1000 [01:24<5:47:26, 20.93s/it]Processing:   0%|▏                                               | 5/1000 [01:44<5:45:47, 20.85s/it]Processing:   1%|▎                                               | 6/1000 [02:05<5:44:25, 20.79s/it]Processing:   1%|▎                                               | 7/1000 [02:26<5:44:10, 20.80s/it]Processing:   1%|▍                                               | 8/1000 [02:46<5:43:00, 20.75s/it]Processing:   1%|▍                                               | 9/1000 [03:07<5:42:03, 20.71s/it]Processing:   1%|▍                                              | 10/1000 [03:28<5:41:36, 20.70s/it]Processing:   1%|▌                                              | 11/1000 [03:48<5:40:53, 20.68s/it]Processing:   1%|▌                                              | 12/1000 [04:09<5:40:23, 20.67s/it]Processing:   1%|▌                                              | 13/1000 [04:30<5:40:04, 20.67s/it]Processing:   1%|▋                                              | 14/1000 [04:50<5:39:31, 20.66s/it]Processing:   2%|▋                                              | 15/1000 [05:11<5:38:53, 20.64s/it]Processing:   2%|▊                                              | 16/1000 [05:32<5:38:47, 20.66s/it]Processing:   2%|▊                                              | 17/1000 [05:52<5:38:01, 20.63s/it]Processing:   2%|▊                                              | 18/1000 [06:13<5:37:21, 20.61s/it]Processing:   2%|▉                                              | 19/1000 [06:33<5:37:22, 20.64s/it]Processing:   2%|▉                                              | 20/1000 [06:54<5:37:11, 20.64s/it]Processing:   2%|▉                                              | 21/1000 [07:15<5:37:05, 20.66s/it]